'Localization의 불확실성'을 반영하려면 환경과 에이전트의 설계를 더 복잡하게 수정해야 합니다.

1. 🌪️ 환경에 '불확실성' 추가하기
가장 먼저, 환경 자체에 노이즈(Noise)를 추가하여 에이전트의 위치 추정을 방해해야 합니다.

Action 노이즈 (Odometry Error): 에이전트가 '직진'(action=0)을 명령해도, 실제로는 바퀴가 미끄러지는 현상을 시뮬레이션합니다.

예시: step 함수 수정

action=0을 받으면, 80% 확률로만 직진합니다.

10% 확률로 의도와 다르게 왼쪽으로 한 칸 이동합니다.

10% 확률로 의도와 다르게 오른쪽으로 한 칸 이동합니다.

Sensor 노이즈 (Measurement Error): 에이전트의 LIDAR 센서가 true_map을 100% 정확하게 읽지 못하게 만듭니다.

예시: _update_belief_map_with_lidar 함수 수정

벽을 스캔할 때, 95% 확률로만 '벽'(1)으로 인식합니다.

5% 확률로 '길'(0)로 잘못 인식합니다.

길을 스캔할 때도 마찬가지로 노이즈를 추가합니다.

2. 🤖 에이전트가 '위치'를 추정하게 만들기
환경에 노이즈가 생기면, 에이전트는 더 이상 self.agent_pos를 믿을 수 없습니다. 이제 에이전트는 **자신의 위치를 스스로 '추정'**해야 합니다.

이것이 문제를 **POMDP(부분 관측 마르코프 결정 과정)**로 만듭니다. 에이전트는 '진짜 상태(true state)'를 모르고, 오직 '관측(observation)'만을 통해 상태를 추론해야 합니다.

State (상태)의 변화:

기존 State: belief_map (나는 내 위치를 정확히 알고, 맵만 모른다)

변경 State: belief_map + position_belief (나는 맵도 모르고, 내 위치도 정확히 모른다)

Position Belief (위치 신념):

에이전트는 "나는 (3, 4)에 60%, (3, 5)에 30%, (4, 4)에 10% 확률로 존재해"와 같은 확률 분포로 자신의 위치를 추정해야 합니다.

이것을 구현하는 고전적인 방법이 바로 **'파티클 필터(Particle Filter)'**입니다. 수백 개의 '입자(particle)'를 맵에 뿌려두고, 각 입자가 에이전트의 "가능한 현재 위치"를 나타내게 하는 방식입니다.

3. 🎯 RL 문제에 반영하기 (보상 및 상태)
Localization을 반영하면 RL 문제가 훨씬 더 복잡해지고, 다음과 같은 수정이 필요합니다.

State (관측) 변경: RL 에이전트(DQN)에 들어가는 State 벡터가 훨씬 더 복잡해집니다.

belief_map (12x12 = 144개)

position_belief (예: 파티클 100개의 (x, y) 좌표 = 200개)

... 이 둘을 합친 거대한 벡터가 State가 됩니다.

Reward (보상) 설계 변경:

기존 보상: (새로 발견한 셀) - (시간 페널티) - (충돌 페널티)

추가 보상: **'위치 불확실성 감소'**에 대한 보상을 추가할 수 있습니다.

예시 (Loop Closure): 에이전트가 불확실한 지역을 탐험하다가, 이전에 확실하게 알았던 '랜드마크'(예: 특정 'ㄱ'자 코너)로 돌아왔다고 가정합시다. 센서 스캔을 통해 "아, 내가 예전에 봤던 그 코너가 확실하네!"라고 깨닫는 순간, 에이전트의 **위치 불확실성이 급격히 감소(파티클이 한 점으로 수렴)**합니다. 이때 큰 보너스 리워드를 줍니다.

이는 에이전트가 '길을 잃지 않기 위해' 주기적으로 아는 곳을 방문하도록 유도합니다.

💡 프로젝트를 위한 현실적인 조언
'Active SLAM (Localization + Mapping)'을 RL로 한 번에 푸는 것은 매우 난이도가 높은 연구 주제입니다. 기한이 정해진 프로젝트(2025.12.7 )에서는 범위를 조절하는 것이 중요합니다.

Plan A (권장): 'Active Mapping'에 집중합니다.

"본 프로젝트에서는 Localization은 완벽하다고 가정하고, Mapping의 효율성(최소 스텝으로 95% 탐색)에 집중한다"라고 보고서에 가정과 한계를 명확히 밝힙니다.

현재 코드를 기반으로 DQN 에이전트를 구현하고, Random Agent 및 Greedy Agent(가장 가까운 미탐색 지역으로 가는 에이전트)와 성능을 비교 분석합니다. 이것만으로도 충분히 훌륭한 프로젝트입니다. 

Plan B (도전): 'Robust Active Mapping'을 시도합니다.

위 1번의 'Action 노이즈'만 간단히 추가합니다. (예: 10% 확률로 미끄러짐)

단, 에이전트는 여전히 **자신의 위치를 완벽히 안다(self.agent_pos)고 '착각'**하게 둡니다.

이런 "노이즈가 낀 환경"에서도 DQN 에이전트가 Random Agent보다 맵을 잘 그리는지(강인함, Robustness)를 실험합니다.

Plan C (매우 어려움): 'Full Active SLAM'을 시도합니다.

Action/Sensor 노이즈를 모두 추가하고, 에이전트가 파티클 필터로 위치를 '추정'하게 합니다.

RL 에이전트(DQN)는 이 파티클들의 분포와 belief_map을 State로 입력받습니다.

...이것은 사실상 대학원 연구 수준의 난이도입니다.

어떤 방향으로 진행하고 싶으신가요? 우선 Plan A를 목표로 하고, Random Agent와 비교할 DQN 알고리즘을 구현해 볼까요?
